# CIFAR-10 图像分类两层神经网络实验报告

## 1. 项目概述

本项目实现了一个两层神经网络分类器，用于CIFAR-10数据集的图像分类任务。整个实现过程完全基于numpy，手动实现了前向传播和反向传播算法，不依赖任何深度学习框架。

### 1.1 CIFAR-10数据集

CIFAR-10是一个包含10个类别的彩色图像数据集，每个类别包含6000张32x32的彩色图像。10个类别分别是：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。数据集分为50000张训练图像和10000张测试图像。

### 1.2 模型架构

本项目实现的两层神经网络架构如下：

- 输入层：3072个神经元（32x32x3的展平图像）
- 隐藏层：可配置大小，默认100个神经元
- 输出层：10个神经元，对应10个类别

激活函数支持ReLU、Sigmoid和Tanh，损失函数使用交叉熵，同时支持L2正则化。

## 2. 实现细节

### 2.1 模块化设计

项目按照模块化设计原则，分为以下几个主要部分：

1. **模型部分**：实现神经网络的前向传播和反向传播算法
2. **数据处理部分**：加载和预处理CIFAR-10数据集
3. **训练部分**：实现SGD优化器、学习率调度和模型训练过程
4. **参数搜索部分**：实现超参数搜索功能
5. **测试部分**：评估模型在测试集上的性能

### 2.2 关键算法实现

#### 2.2.1 前向传播

前向传播过程计算从输入到输出的整个网络流程：

1. 第一层线性变换：Z1 = X·W1 + b1
2. 第一层激活：A1 = activation(Z1)
3. 第二层线性变换：Z2 = A1·W2 + b2
4. Softmax输出：Y = softmax(Z2)

#### 2.2.2 反向传播

反向传播算法通过链式法则计算损失函数关于网络参数的梯度：

1. 输出层误差：dZ2 = Y - T (T为one-hot编码的标签)
2. 输出层权重梯度：dW2 = A1.T·dZ2
3. 输出层偏置梯度：db2 = sum(dZ2, axis=0)
4. 隐藏层误差：dA1 = dZ2·W2.T
5. 隐藏层激活函数梯度：dZ1 = dA1 * activation_derivative(Z1)
6. 隐藏层权重梯度：dW1 = X.T·dZ1
7. 隐藏层偏置梯度：db1 = sum(dZ1, axis=0)

对于L2正则化，每个权重的梯度还需加上正则化项的梯度：λ*W。

## 3. 实验结果与分析

### 3.1 超参数搜索

超参数搜索探索了以下几个关键参数的影响：

1. **学习率**：尝试了[0.001, 0.01, 0.05, 0.1, 0.5]
2. **隐藏层大小**：尝试了[50, 100, 200, 300, 500]
3. **正则化系数**：尝试了[0.0, 0.0001, 0.001, 0.01, 0.1]
4. **激活函数**：尝试了[relu, sigmoid, tanh]

### 3.2 训练曲线

下面是使用最佳超参数训练模型的训练曲线：

![训练曲线](./cifar10/results/training_curves.png)

### 3.3 权重可视化

第一层权重可视化（重新塑形为图像）：

![第一层权重](./cifar10/results/layer1_weights_images.png)

### 3.4 测试结果

在测试集上的分类准确率：

![混淆矩阵](./cifar10/results/confusion_matrix.png)

## 4. 结论与改进方向

### 4.1 结论

本项目成功实现了一个基于numpy的两层神经网络，能够有效地对CIFAR-10数据集进行图像分类。通过手动实现反向传播算法，加深了对神经网络工作原理的理解。

### 4.2 改进方向

1. **网络深度**：增加网络层数可能提高模型表达能力
2. **优化算法**：实现更高级的优化算法，如Adam、RMSprop等
3. **卷积层**：针对图像任务，使用卷积层可能取得更好的效果
4. **数据增强**：使用数据增强技术扩充训练集
5. **批归一化**：添加批归一化层可能提高训练稳定性和收敛速度

## 5. 参考资料

1. CIFAR-10数据集：https://www.cs.toronto.edu/~kriz/cifar.html
2. 神经网络与深度学习：http://neuralnetworksanddeeplearning.com/
3. 反向传播算法：https://en.wikipedia.org/wiki/Backpropagation
